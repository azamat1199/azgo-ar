<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <link rel="icon" href="%PUBLIC_URL%/favicon.ico" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="theme-color" content="#000000" />
    <meta
      name="description"
      content="Web site created using create-react-app"
    />
    <link rel="apple-touch-icon" href="%PUBLIC_URL%/logo192.png" />
    <link rel="manifest" href="%PUBLIC_URL%/manifest.json" />
    <title>React App</title>

    <script src="https://aframe.io/releases/1.2.0/aframe.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/jeromeetienne/ar.js/aframe/build/aframe-ar.min.js"></script>
  </head>
  <!-- <body>
    <noscript>You need to enable JavaScript to run this app.</noscript>
    <div id="root"></div>

  </body> -->
  <body style="margin: 0; overflow: hidden">
    <!-- AR Canvas -->
    <div id="ar-canvas"></div>

    <!-- Webcam -->
    <video
      id="webcam"
      autoplay
      playsinline
      style="
        position: absolute;
        top: 0;
        left: 0;
        width: 100%;
        height: auto;
        z-index: -1;
      "
    ></video>

    <script>
      let scene, camera, renderer, video, model, mesh;

      async function init() {
        // Initialize Three.js scene
        scene = new THREE.Scene();

        camera = new THREE.PerspectiveCamera(
          75,
          window.innerWidth / window.innerHeight,
          0.1,
          1000
        );
        camera.position.z = 3;

        renderer = new THREE.WebGLRenderer({ antialias: true });
        renderer.setSize(window.innerWidth, window.innerHeight);
        document.getElementById("ar-canvas").appendChild(renderer.domElement);

        // Add lighting
        const light = new THREE.DirectionalLight(0xffffff, 1);
        light.position.set(0, 1, 1).normalize();
        scene.add(light);

        // Access webcam
        video = document.getElementById("webcam");
        const stream = await navigator.mediaDevices.getUserMedia({
          video: true,
        });
        video.srcObject = stream;

        // Load TensorFlow.js model
        model = await cocoSsd.load();
        console.log("COCO-SSD model loaded!");

        // Create an empty placeholder object
        const geometry = new THREE.BoxGeometry();
        const material = new THREE.MeshLambertMaterial({ color: 0xff0000 });
        mesh = new THREE.Mesh(geometry, material);
        mesh.visible = false; // Hide initially
        scene.add(mesh);

        // Start detection and rendering loop
        detectObjects();
        render();
      }

      async function detectObjects() {
        // Wait for the video to be ready
        if (!video.readyState === 4) {
          requestAnimationFrame(detectObjects);
          return;
        }

        const predictions = await model.detect(video);
        console.log(predictions);

        if (predictions.length > 0) {
          const obj = predictions[0]; // Take the first detected object
          console.log(
            `Detected object: ${obj.class} with ${Math.round(
              obj.score * 100
            )}% confidence.`
          );

          // Render the detected object as a 3D box
          mesh.visible = true;
          mesh.scale.set(1, obj.bbox[3] / 200, obj.bbox[2] / 200);
          mesh.position.set(obj.bbox[0] / 200 - 1.5, obj.bbox[1] / 200 - 1, -3);
        } else {
          mesh.visible = false; // Hide the box if nothing is detected
        }

        // Continue detection
        requestAnimationFrame(detectObjects);
      }

      function render() {
        requestAnimationFrame(render);
        renderer.render(scene, camera);
      }

      init();
    </script>
  </body>
</html>
